---
layout: post
title: 关于最近人工智能的探索
tags: [AI, LLM, 人工智能]
---

  最近人工智能发展的还真是不错啊……<!--more-->    
  
# 起因
  最近ChatGPT为代表的人工智能发展的越来越好了，而且因为它对生产力的提升使得了解AI的人也越来越多了。虽然我也不算是对AI很感兴趣，但是我在Github Copilot刚出的时候就已经用上了，到现在一直在用（不过毕业了以后估计就用不了了吧😂）。不过那时候Copilot毕竟专业性比较高，知道的人也比较少，不像现在ChatGPT能在各行各业使用，甚至还有基于类似模型的Vtuber，比如[Neuro-sama](https://www.twitch.tv/vedel987)，所以即使是普通人使用它，都能够减轻自己的工作压力，所以现在的人们都在讨论它。   
  当然在这之前，还有一些很厉害的画图AI，比如使用了Stable Difusion的NovalAI，以及Midjourney啥的，不过因为我对画图并不感兴趣，所以它发展的有多好也基本上和我没有关系。其实除了这些能够AIGC的模型之外，在那之前还有下围棋的AlphaGO啥的，那个我就更不感兴趣了，相信大多数人也不感兴趣，所以总的来看也就只有现在才能证明AI发展到了能够让大家觉得能干涉到更多人的地步吧。   
  也正因为现在以ChatGPT为代表的LLM的发展，开源社区也开始搞起来一些有意思的东西。不过LLM的训练成本比较高，所以现在开源社区在这一块的发展也许得感谢比如Facebook的[LLaMA](https://github.com/facebookresearch/llama)之类基础的模型，才能让大家能用较低的成本去训练属于自己的AI吧。

# 关于LLaMA衍生的模型体验
  在刚开始LLaMA被Facebook开源的时候，GitHub上就出现了[llama.cpp](https://github.com/ggerganov/llama.cpp)这个项目，因为我没钱买显卡，手头只有笔记本电脑上的一张非常垃圾的GTX1650Ti 4GiB显存的显卡，多亏了这个项目，可以让我这种没显卡的人也能体验LLM的乐趣。而且这个项目使用起来非常简单，不像很多AI项目还要装什么TensorFlow啊，还是什么PyTorch啥的，那些东西不仅大，还非常的挑版本，这个项目运行的时候就什么都不用考虑，对使用者来说非常的友好，像清华那个[ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)我就完全跑不了，它要想正常体验得要有8GiB的显存，我就没法整这种东西了。   
  我体验了那个最开始的LLaMA-7B的模型，效果其实不怎么样，根本没法流畅对话，不过也能理解，因为它应该相当于是把一堆数据堆到一起的东西吧，也没有针对对话进行训练。不过很快，斯坦福大学对这个模型进行了Finetune，制作出了[Alpaca](https://github.com/tatsu-lab/stanford_alpaca)，当然这个仓库里面的东西是纯粹的菜谱和食材，把他们加工成模型得要整一堆A100的显卡跑几个小时，这个我可整不来，不过还好有人根据这个原材料加工成了完整的模型，现在去🤗上就能下载的到，比如去[这个仓库](https://huggingface.co/chavinlo/alpaca-native)就能下载到训练好的模型，在[这里](https://huggingface.co/Pi3141/alpaca-native-7B-ggml)可以下载到已经经过量化，直接可以给llama.cpp使用的版本（不过现在llama.cpp升级了，得要按照说明在仓库里执行那个Python脚本进行转换才能正常使用）。   
  我运行这些东西是在我的Macbook Pro上，它只有8GiB的内存，所以只能跑7B（70亿参数）的模型，不过就这个模型也已经非常厉害了，虽然只能说英文，但是流畅程度，上下文关系的能力都非常不错，而且常见的知识都能正确回答，效果让我很满意。不过8GiB内存实在是太小了，想跑点别的也没办法……不过16GiB内存的电脑我还是能找得到的，我找了台CPU是i7-11700K的台式机跑了下[使用GPT-4对话数据微调的13B模型](https://huggingface.co/Pi3141/gpt4-x-alpaca-native-13B-ggml)，速度比M2芯片跑的速度感觉慢了至少5倍吧，Macbook生成的速度基本上能达到对话的语速，台式机跑的速度那就是一个一个词往出蹦，这下就能感受到M2芯片的计算能力还是强大啊，4大核+4小核比8核16线程还要厉害，苹果的产品属实有点东西😝，不过这个13B的模型也确实厉害，有些7B的模型回答有错误的部分这个13B的都能正确回答，如果我能整个16GiB的Macbook可能就能完整体验了吧，可惜苹果家的内存比金子都贵，实在是买不起啊……   
  斯坦福的这种Finetune方法成本还是有点高，所以后来又有人研究出了一个叫[LoRA](https://arxiv.org/pdf/2106.09685.pdf)的办法去Finetune模型，据说只要一张普通的显卡就可以进行，不需要一堆高级显卡也能跑，不过再低级的显卡，也不是我这GTX1650Ti能碰瓷的，所以我也只能看看别人训练的模型啦。在看的时候我发现了一个用的中文训练集跑的模型项目[Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)，看起来还挺有意思的，不过他们似乎担心什么版权问题，只放出了LoRA权重，没有完整的模型，就相当于是没有面饼的泡面，只有料包，好在他们倒是给了去哪里找面饼。不过泡面的这个过程对我来说也挺难的，合并它需要13-15GiB的可用内存，16GiB的内存肯定不够用，所以得找一台32GiB内存的电脑（不会有人组非2次方倍数内存的电脑吧？）……我手头没有，还好从网上找了台免费的云主机，整了个32GiB内存的，才成功的把这碗面泡出来了。虽然泡面的步骤不多，也不算特别难，只是我是觉得干嘛同一碗面泡那么多次，不如我提前泡好直接放网上，所以我也在🤗上上传了最终合并并量化的模型，[在这里](https://huggingface.co/Mabbs/chinese-Alpaca-lora-7b-ggml)就能下载到直接就能拿来用的模型了。至于运行的参数就按照之前仓库的操作就行了。   
  
# 让M2芯片发挥更大的作用
  在我玩完那个llama.cpp项目之后，我觉得让M2芯片光用CPU算好浪费啊，毕竟这个芯片里面还有神经网络引擎和GPU啊，这些哪个都比CPU算更好吧，不过想要调那些东西进行人工智能计算貌似只能用苹果的Core ML框架。这两天我在Github看到一个苹果官方发布的[Core ML Stable Diffusion](https://github.com/apple/ml-stable-diffusion)，看起来还挺有意思的，我倒是也想在我的Macbook上跑一下，正好🤗开发了一个示例[Diffusers](https://apps.apple.com/app/diffusers/id1666309574)可以拿来试试看，可惜8GiB内存还是限制了它的发挥，跑是能跑，速度也挺快的，就是模型没法换，因为内存也只能跑小的模型，跑出来可以说真的就是那种抽象的不能再抽象的东西吧……不过不管怎么说，这还是发挥了一下神经网络引擎和GPU的作用吧，不然感觉苹果做了这些东西就没啥作用了😂。   
  
# 感想
  从这次体验来看，我对开源社区的AI发展感觉还是挺有信心的，虽然相比于ChatGPT之类的来说可能还没办法当作生产力，不过毕竟它比较平民化，至于知识量少的问题如果开发者们能加把油能让这些模型对接网络，那也许就可以在很低成本的情况代替OpenAI的ChatGPT吧，也能避免他们服务器出问题之类的原因导致生产力的下降吧。不过开源这种事情还是有点……毕竟还是不希望这些东西被人拿去商业化，更不希望出现国外一开源，国内就自研这种糟糕情况，也许开源产品比不上人家的商业产品也是正常的。
   
    
