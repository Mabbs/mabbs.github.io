---
layout: post
title: 近期LLM的部署与应用经历
tags: [LLM, AI, 人工智能]
---

  玩AI开始变的有些烧钱了啊……<!--more-->    

# 起因
  在几年前我就已经[探索并玩过很多LLM了](/2023/04/05/ai.html)，不过近些日子在这方面的发展似乎影响到了我的生活……由于近期某公司开发的DeepSeek在国内非常火，导致我也不得不跟上这个热潮去考虑怎么应用它。当然对于普通人来说，使用它并没有什么难度，即使DeepSeek的官方网站和APP现在基本不能用，现在各家大公司也都自行搭建了，目前我感觉使用DeepSeek体验最好的是百度，其他家使用无论是可用性还是速度都比不过百度，而且目前百度也没有限制使用量之类，还是挺不错的。   
  但是对我来说却不能直接使用其他公司的产品，其实要从成本来说接入其他公司的接口显然是要便宜的多，但是我需要应用的地方可能连不上那些接口😅，所以需要考虑自己搭建。   

# 部署经历
  为了能自己搭建DeepSeek，首先就得买硬件了……虽然前段时间[整了台新服务器](/2025/02/09/server.html)，但是让CPU来跑还是太吃力了，速度太慢了……所以为了能轻松的跑起来，最近整了张RTX4090 48GiB显存魔改版（但是手头没有空闲的机器了，只能插在一台用着[i5-8400](https://www.intel.cn/content/www/cn/zh/products/sku/126687/intel-core-i58400-processor-9m-cache-up-to-4-00-ghz/specifications.html)处理器的主机，这下成狗骑吕布了🤣）。有了这张显卡，跑DeepSeek-R1的蒸馏模型（从1.5B到70B的Q4_K_M量化版）倒是轻轻松松，用Ollama跑70B的模型也能到20Tps的速度。但是根据测试来看，这些蒸馏模型的效果很差，基本上没法用，这些模型经常会发生不遵守指令，内容随机掺杂英文，而且也经常发生逻辑错误，和671B的完整版完全不能比，用起来还不如Qwen2.5各规模的模型。   
  那怎么办呢？前几天清华大学的某个团队更新了一款叫做[KTransformers](https://github.com/kvcache-ai/ktransformers)的框架，据说它可以利用Intel的AMX指令集然后配一张RTX4090可以让DeepSeek-R1 671B Q4_K_M量化版跑到13Tps，能跑到这个速度那至少是可用级别了，调其他公司的接口基本上也就是这个速度，之前买的新服务器不就有这个指令集嘛（之前还感觉这个指令集有点鸡肋呢，看来还是开发度不够啊😆），如果再配一个CPU，然后把内存插满也许就可以了？可惜R760xs插不了全高的显卡，要想插全高的估计就只能买R760了，或者用PCI-E延长线？不过那样感觉不太可靠……不过之后肯定还是会想办法上完整版的模型，毕竟它的效果确实是不错，最关键的是它的市场认可度高，上了就能提高产品竞争力，所以之后应该会想办法搞到满足KTransformers的硬件然后跑起来，或者等[llama.cpp](https://github.com/ggml-org/llama.cpp)合并它的算法，然后用llama.cpp会更好一些。   
  不过我更倾向于等Mac Studio M4 Ultra出来，应该过几个月就能出，按照目前发展趋势来看，新款Mac Studio应该会有更大的内存，理论上可以跑的动一些效果更好的[动态量化版](https://unsloth.ai/blog/deepseekr1-dynamic)（现在能在M2 Ultra上跑的那个1.58位的效果还是不太行），相比于价格十几万的服务器，Mac Studio估计不到十万，可以说是非常有性价比了。当然如果等不及的话应该还是会选择花十几万买个有双路第四代至强可扩展处理器加512GiB内存的服务器吧……   

# 应用经历
  有了模型之后如果只是聊天那就没必要费这么大劲了，费劲搭当然是为了能让它参与到实际的工作当中。不过该如何应用它呢？首先要让它知道工作的内容，所以第一步要搞出知识库。知识库的原理倒是很简单，我之前就给我博客的[聊天机器人加了RAG功能](/2024/09/27/rag.html)，核心就是嵌入模型和向量数据库。不过我写的那个全都是为了能使用Cloudflare的功能，脱离了Cloudflare就没用了。那如果要在本地搞应该怎么办呢？我之前用过的[1Panel](/2024/02/03/1panel.html)开发它的公司旗下有个叫[MaxKB](https://github.com/1Panel-dev/MaxKB)的产品看起来很不错，它使用了PGSQL和[pgvector](https://github.com/pgvector/pgvector)作为向量数据库来搭建知识库，而且它是用Python写的，还能用Python来写自定义功能的函数库，另外它还能用可视化的方式来设计工作流，可以轻松构建需要的逻辑，从功能上来说我还是挺满意的。   
  使用也挺简单，在设置里可以添加使用其他公司API的模型，也可以使用Ollama，不过这一步有个坑，Ollama并不支持设置API Key，但是它添加模型却要求配置一个API Key，文档说可以输入任意内容，我输了一个空格，可以保存，但是使用的时候会报网络错误，所以它文档里怎么不说明一下是除了空格之外的任意内容😅，浪费了我不少时间。   
  在添加知识库的时候可以除了[内置的嵌入模型](https://github.com/shibing624/text2vec)（好像是腾讯的员工搞的模型），也可以用Ollama的嵌入模型。它自带的嵌入模型用的是CPU，文档规模大的情况速度比较慢，因为在Cloudflare上我用的是BAAI的BGE模型，效果还可以，所以这次我还是选了它，但是选的是中文模型，这样就不需要再翻译了🤣。   
  开始我对MaxKB印象还是挺不错的，但是用着用着……在建第六个应用的时候它显示社区版只能创建五个应用😅，对于开源软件这样做限制我也是大开眼界了，要是说有些专业版功能不开源，是DLC的形式，付钱来获取更多的功能代码，我还能理解，在开源代码上做数量上的限制，这垃圾公司多少有点看不起人了😅。   
  那对于这种挑衅行为该怎么反制呢？它的代码倒是没有混淆之类的，还算不错，比我以前用过的[KodExplorer](https://github.com/kalcaddle/KodExplorer)要好，它还整个“部分开源”，有个[关键文件](https://github.com/kalcaddle/KodExplorer/blob/master/app/controller/utils.php)直接是混淆过的，想改都改不了😅，至少MaxKB还能随便改。   
  我大概看了眼代码，只需要改两个文件就行，一个是“apps/common/util/common.py”，把其中“valid_license”函数进行判断的部分全部注释，另外一个文件是“apps/setting/serializers/valid_serializers.py”，把“ValidSerializer”方法中的“valid”方法里进行判断的部分全部注释就可以了，开源还做限制我是真的无法理解……   
  如果是用1Panel部署的，可以把那两个文件放到“/opt/1panel/apps/maxkb/maxkb”目录下，然后在docker-compose.yml文件的volumes段添加：   
```yml
- ./common.py:/opt/maxkb/app/apps/common/util/common.py
- ./valid_serializers.py:/opt/maxkb/app/apps/setting/serializers/valid_serializers.py
```
  就可以了。   
  不过总体来说从功能上我还算比较满意，就原谅它搞出这种奇葩的行为吧😆。   
  MaxKB主要是为了能给更多人使用，所以是网页版，部署也略显麻烦，如果是自己用呢？我之前看到过一个桌面软件，叫做[Cherry Studio](https://github.com/CherryHQ/cherry-studio)。它更适合开箱即用一些，功能上可能不如MaxKB强大，但是比较方便一些。比如上传文档，MaxKB需要在流程图中自行处理，这个软件会帮你处理好；添加知识库可以直接添加本地的文件夹，不用上传到服务器上；另外安装比较方便，不像MaxKB搭环境比较麻烦些，所以个人用的话可以用Cherry Studio。   

# 感想
  总的来看，DeepSeek的出现还算可以，虽然它受到的关注和它的能力也许并不匹配，但是毕竟现在的它已经是人人都能蹭的东西了，谁都能挂它的名头，我们来蹭一蹭也能分点它的好处。当然这样的结果倒也不差，开发DeepSeek的公司只能获得他们应得的部分，其他的关注度就应该被各家公司瓜分😆。我在这期间虽然很难获得什么实质性的收获，但是能在这期间能搞点很贵的硬件之类的玩玩也是不错的体验啊🤣。   