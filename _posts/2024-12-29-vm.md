---
layout: post
title: ESXi和PVE的使用体验与对比
tags: [ESXi, PVE, 虚拟机]
---

  装虚拟机用什么系统更好呢？<!--more-->    

# 起因
  前段时间我有个需要开很多机器的需求，为了方便管理和提高资源利用率，当然是上虚拟机比较合适。那用什么系统上虚拟机好呢？Windows上用Hyper-V当然也是不错的选择，但是我觉得Windows的基础占用太高了，另外Hyper-V的操作面板也不怎么样，所以就不考虑了。那用什么呢？之前我上大学的时候用过ESXi，在随身携带的U盘里上正好有一份，一直没删，所以就顺手给手头的工作站安了一下。不过我当时用的版本很旧了，是6.7，虽然也不是不能用，但是考虑到这个版本之前有RCE漏洞，所以去sysin上下了一份最终版的6.7U3u更新包更新了上去，以后就不再更新了。   
  不过除了ESXi之外还有别的选择，我看很多人都拿PVE和ESXi比较。虽然经常听说PVE但是我没有用过，所以就在另一个工作站上安装了PVE试试看哪个用起来更好。不过和PVE比的其实不该是ESXi，而是VMWare vSphere，只不过我两个系统都是一台机器，也用不着搞集群，找破解版还麻烦。所以其实我是拿ESXi的VMware Host Client和PVE进行对比。   
  另外从本质来说它们也不是一个东西，PVE更像是Debian上一个管理虚拟化的面板，ESXi是VMKernel附带了个可以临时使用的Web端面板，侧重点不一样。   

# ESXi和PVE的对比
## 界面与体验
  首先从界面来看两个系统长得其实差不太多，不过左侧导航栏有点不太一样，把PVE的导航栏改成文件夹视图就和ESXi的差不多了。从界面上来说我更喜欢ESXi的界面，PVE的感觉没什么设计感。不过PVE面板的数据是1秒刷新一次的，ESXi就算配置刷新也只能最短每15秒刷新一次。从功能上来说可能PVE会更好一点。另外对于显示的图表来说PVE全在“概要”里，在ESXi都在“监控”里，虽然PVE的图表更多，但是有些感觉没什么意义，因为PVE是基于Linux的，所以有“负载”这个指标，不过对于虚拟机系统来说感觉意义不大啊……不过也可能是因为用了LXC容器之后会影响PVE的负载所以整了这个项目？   
  另外PVE还有个好处是可以看CPU温度，我看有一个叫“[pvetools](https://github.com/ivanhao/pvetools)”的工具可以配置在界面上显示CPU频率和温度，ESXi没有IPMI的话用啥办法都看不到CPU温度😅。
## 虚拟机管理
  ESXi和PVE创建虚拟机都挺简单的，都有专门的向导。不过我测试PVE的时候选择安装Windows 10，它推荐的架构居然是i440fx机器和SeaBIOS固件，虽然也不是不能用，但它怎么选了个最垃圾的，虽然选成Windows 11是推荐的q35和UEFI引导……而且SCSI控制器还选了个要驱动的半虚拟化设备，但PVE没有自带这个驱动包啊，这些都是不小的坑。而ESXi就正常多了，选择Windows 10会默认使用UEFI引导，而且会选择一个兼容性最好的SCSI控制器和网络适配器，便于没有安装驱动的时候能正常使用，另外ESXi是自带VMWare Tools的，在系统安装完成后可以直接挂载安装，比PVE的体验好很多。另外PVE还有一个坑，那就是CPU默认会用QEMU自己的一个类型，那个在虚拟机里就读不到CPU的型号了，而且性能会打折扣。不过这个倒也能理解，毕竟PVE是给集群设计的，在迁移到其他节点的时候选host可能在迁移到不同CPU的节点时会出现问题。不过ESXi也是啊……怎么它就没有这种问题？总之PVE不适合小白。   
  PVE相比ESXi多了个特性，那就是LXC容器，因为PVE是基于Linux的，所以可以创建容器。这个体验倒是还行，可以直接在面板上下载模版，创建也没什么坑，配好之后和虚拟机几乎一模一样，甚至还能在上面安装Docker，IP也是独立分配的，用起来还不错。   
## 存储管理
  PVE相比ESXi在存储上能选的花里胡哨的东西有点多，默认它会把系统盘配置成LVM，然后单独分了个LVM-Thin的东西，两个容量不互通。这个LVM-Thin好像是只能用来存磁盘，而且看不到东西到底存在哪里了，我搜了一下好像是说这个LVM-Thin可以用多少占多少空间……我寻思qcow2格式的磁盘也有这个功能啊，而且raw格式的磁盘文件是稀疏文件，也是用多少占多少啊……两个容量不互通还浪费磁盘空间，然后我就把这个LVM-Thin删掉了，把系统盘扩容到整个磁盘，然后在存储里面允许local存储磁盘映像之类的。   
  除此之外PVE还支持ZFS，相当于软RAID，但是它是文件系统层面支持的，不需要初始化。我手头有3块机械盘，插在上面组了个RAIDZ，可以允许坏任意1块。组好之后可以用来存储磁盘映像和容器的数据。   
  ESXi的话就只能把盘格式化成VMFS6的文件系统，要么还能挂iSCSI当作磁盘或者NFS作为数据存储，如果要分布式存储应该只能搭到别的机器上然后用iSCSI挂过来，阵列看起来只能是硬RAID，ESXi并不提供软RAID的选项，也不支持挂SMB、CephFS、ZFS之类乱七八糟的东西，PVE在这一方面因为基于Linux系统发挥了很大的优势，只要Linux能挂的它就能挂。   
## 网络管理
  在PVE上的网络是用的Linux Bridge，安装的时候会强制要求静态IP，不过毕竟是Linux，可以修改配置让它使用DHCP。不过看起来PVE上似乎没有配置NAT的选项，当然作为Linux来说肯定是有办法配的。ESXi用的叫做虚拟交换机，配置冗余也是在这里配置，PVE的话应该要先配置Bond然后再配置网桥。   
  另外ESXi对网卡要求很高，不是服务器或者工作站，用的比如什么瑞昱的网卡都是不识别的，要额外安装驱动才行，这也是PVE的优势，Linux兼容什么，它就兼容什么。不过对于大公司来说，也不可能用家用电脑当服务器使🤣，所以就算是用ESXi也不存在这种问题。   
## PCI直通
  在这一方面ESXi的体验就比PVE要好很多，直接在“管理”——“硬件”——“PCI设备”里面就可以配置显卡直通之类的，没有什么复杂的配置，直接点“切换直通”然后重启就可以在虚拟机里配置了（当然VT-d之类的东西要提前开）。   
  PVE我最开始配直通的时候是直接网上搜的，那个pvetools也可以帮助配置PCI直通之类的，用这个工具配完之后就可以在虚拟机里添加了。不过在我添加的时候发现它有个“映射的设备”这个选项，用刚才那个工具配置完之后要选“原始设备”，然后我就想着这两个有什么区别，结果发现“数据中心”——“资源映射”里面有个PCI设备的选项😂，也许从一开始我就做错了，直接用这个添加就可以了吧？只不过因为我已经用那个工具配置过了，怕在这里加了会冲突，所以就算啦。   
  另外PVE的PCI直通还有个好处就是在5-10代的IntelCPU可以用Intel GVT-g技术把核显拆成多个显卡，像虚拟机如果要是需要显卡的话用这个就不用插一堆显卡给虚拟机分配了。ESXi的话只支持SR-IOV拆分，这个只有11代之后的Intel核显才可以用……我用的这两台工作站都是Intel6代的U，所以在ESXi只能把整个核显直通分给某台机器了……   
## 硬盘直通
  硬盘直通有两种方式，一种是把控制器直通了，另外是只直通某个磁盘，在ESXi上叫RDM直通。我的主板只有一个SATA控制器，而且没有NVME硬盘，所以直通控制器肯定不行，这样会导致虚拟机管理系统读不到硬盘然后挂掉，所以要直通就只能直通某个硬盘。   
  ESXi直通硬盘有点复杂，要打开SSH，然后用命令创建RDM磁盘文件，挂载到虚拟机就可以了。不过我操作的时候不知道为什么网页出BUG了，加载磁盘文件之后什么都读不到，然后也不能保存，最后没办法只能修改vmx文件进行挂载了……   
  PVE的话我感觉它的直通更像是把整个硬盘的设备文件作为一个磁盘映像来挂载到虚拟机上了，但是PVE不允许在界面上挂载在指定存储以外的文件，所以就只能通过命令来挂载……   
  两个从功能来说都没问题，不过PVE挂载完之后磁盘显示的是“QEMU HARDDISK”，而且SMART信息是瞎编的，ESXi挂载之后可以看到磁盘名字、序列号，另外SMART信息也能看到（至少我用的ESXi 6.7U3u是可以的）。不过PVE可以在面板上看SMART信息，ESXi就只能登SSH敲命令看了……不过要是有IPMI应该也是能获取到硬盘的健康信息的。   

# 总结
  从上面来看PVE的功能是要更多的，但是使用起来不如ESXi友好，坑也比较多，对于不想花时间解决问题的人来说用ESXi会更好一些，当然ESXi也并不是免费产品，它是VMWare vSphere的一个组件，VMWare vSphere是收费的，而PVE是免费的，可以付费获得额外的更新和服务，对于个人而言当然无所谓，两个肯定都不会有个人花钱买，至于公司的话……大公司选择VMWare vSphere当然会更好一些，肯定对运维会很友好，PVE的话小公司免费用会更合适一点。   
  至于哪个我觉得更好……我还是更倾向于用ESXi，虽然PVE功能很多，但是毕竟PVE底层是Linux，我怕乱配给配崩了🤣，ESXi的话就没有那么多会让用户搞坏的地方，所以更稳定啊。