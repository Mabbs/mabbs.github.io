---
layout: post
title: 新旧服务器的使用体验与对比
tags: [服务器, Dell, 使用体验]
---

  花更多钱可以收获更多吗？<!--more-->    

# 起因
  最近由于某些原因需要买点服务器，从我平时用的东西来看，其实很多年前的产品就已经满足大多数应用了，业务的发展跟不上时代的发展，就根本不需要更好的性能。所以既然要买服务器，还是买洋垃圾比较好，那些淘汰下来的服务器特别便宜。虽然这么说，但是我也好奇现在的技术到底发展到一个什么样的程度，所以也整个新的服务器玩玩吧。   

# 选择服务器
  那选哪个服务器比较合适呢？我在大学里用过R730，那款服务器给我留下的印象很不错，拆装很方便，也有很好用的带外管理功能（iDRAC），现在的R730已经非常便宜了，我看了看CPU觉得既然洋垃圾很便宜，那就要选个厉害的CPU，最终我选择了双路20核40线程的[英特尔® 至强® 处理器 E5-2698 v4](https://www.intel.cn/content/www/cn/zh/products/sku/91753/intel-xeon-processor-e52698-v4-50m-cache-2-20-ghz/specifications.html)，总共40核80线程，另外配了4根32GiB 2400MT/s的DDR4内存，看起来参数还是挺唬人的🤣，而且价格才2k多CNY，感觉还挺不错。   
  那新的用啥呢？我上Intel的官网看了看，至强6是现在最新的Intel服务器CPU，至于AMD的……主要是给我买服务器的人不喜欢AMD🤣，所以只能选Intel的。既然旧的选了Dell，新的也选Dell吧，我看搭载至强6的戴尔服务器是R770，但是目前还买不到😅，而且价格贵的吓人。次一级就是R760，可以上第四或第五代至强可扩展处理器，不过看了一眼价格也有点贵……但这个机器有个青春版，叫R760xs，也能上第四或第五代至强可扩展处理器，扩展性稍微差一点，但是价格比较便宜，他们管这叫“成本优化版”。最终选来选去选了个单路16核32线程的[英特尔® 至强® Gold 6426Y 处理器](https://www.intel.cn/content/www/cn/zh/products/sku/232377/intel-xeon-gold-6426y-processor-37-5m-cache-2-50-ghz/specifications.html)，外加4条16GiB 4800MT/s的DDR5内存，总共花了将近4wCNY，感觉还是相当贵啊……    

# 使用体验与对比
  服务器拿到手之后自然要先跑个分，我给新服务器安装了Ubuntu Server 24.04，旧的因为核心数多感觉应该能干点别的所以安装了Vmware ESXi 6.7，然后在上面安装了个Ubuntu Server 24.04的虚拟机。跑分用的是sysbench。最终新的服务器单核跑分2853.45events/s，多核47054.35events/s，旧服务器单核876.22events/s，多核52792.15events/s。从这里来看这个新服务器让人非常失望啊，单核才3倍多点差距，尤其我试了试13代i5的单核跑分能到4290.80events/s，家用的处理器可是要便宜的多啊。多核虽然说16核比40核少了点，能跑出差不多的分数已经很厉害了，但是考虑到这两个服务器20倍的价格差，还是深深的感到不值啊……   
  当然服务器的性能并不是它的优势，扩展性才是，但是R730的定位比R760xs的定位要高啊😂，扩展性显然是旧服务器更强……那新服务器就没什么优势了吗？倒也不是，新服务器的处理器至少把漏洞都修完了，除了幽灵漏洞之外，至少不受其他漏洞影响，安全性更强了。旧处理器和酷睿5代是同一个时代的，所以会受各种CPU漏洞的影响。不过这个服务器又不会当云服务器租给别人用，有没有漏洞根本无所谓啊😅。   
  那管理性呢？新的带外管理用的是iDRAC9，旧的是iDRAC8，两个界面上差距倒是挺大的，不过功能基本上都差不多，从功能上来看9比8多了个修改BIOS的功能，但是修改完还是得重启才能生效😅，那不如花几十块钱买个企业版订阅然后用虚拟KVM直接重启进BIOS修改呢……不过如果是大规模的话可能是可以统一修改BIOS选项，那就有点意义了，不过对我来说没啥意义😥。   
  那还有别的优势吗？我看网上说第四、第五代至强可扩展处理器新出了个指令集，叫AMX，可以用来加速AI推理，正好最近国内一个叫DeepSeek-R1的模型挺火的，那就拿来试试看呗，要是这个AMX指令集能大幅提高CPU的推理速度，那我还是挺认同它的价格的，毕竟内存可以随便加，显存……都被老黄垄断了，价格巨贵无比😂。现在的[llama.cpp](https://github.com/ggerganov/llama.cpp)已经支持了AMX加速，具体的使用方法可以看Intel官网上的[论文](https://www.intel.cn/content/www/cn/zh/content-details/791610/optimizing-and-running-llama2-on-intel-cpu.html)，看起来需要安装Intel oneAPI的库才能编译使用。我折腾了一下编译完跑了一下DeepSeek-R1 32B Q4_K_M蒸馏版，速度大概是5.2token/s。然后我安装了个[Ollama](https://ollama.com/)，它自带的这个llama服务器只支持AVX2指令集加速，但是我试了一下速度能达到4.8token/s，也就是说AMX指令集加速了个寂寞，几乎没起倒什么作用，难怪没什么人讨论。不过我也听说纯CPU跑大模型主要瓶颈在内存带宽上，我插4条也就是四通道，其实也不是它的全部实力，它最大支持八通道，也许给它插满效果会好一些吧……    
  那旧服务器呢？我倒也试了一下，用Ollama跑一样的模型大概是2token/s多的速度，也就是说新的相比旧的也只快了1倍多一点，而且旧的每个CPU只有2条内存，只有双通道，速度也只有新的一半，结果新的才领先了一倍多一点，都上了那么多黑科技……看来Intel是真不行了。   
  当然5.2token/s的速度显然是无法接受的，还是有点慢了，再加上DeepSeek-R1还有思维链，在回答问题前还要生成一堆废话，那就更慢了（其实要我说它那个思维链其实就是把之前的AutoGPT的结果作为训练材料训练的，相当于集成到模型里了，我自己测了一下水平还是不够用，包括官网的满血版也一样）。我之前听说有一种叫做“投机采样”的推理加速技术，不知道为什么凉了，llama.cpp编译的产物里还有这个技术的PoC。于是我就下了个DeepSeek-R1 7B Q4_K_M蒸馏版，拿来试试看用它来加速32B的怎么样。首先我单独测试7B的速度可以达到20token/s，然后我用“llama-speculative”测了一下，感觉有点一言难尽……一阵快一阵慢的，总体来说感觉不如直接跑的快，难怪这个技术凉了😥，不过也可能是因为这两个模型的什么token分布不太一致，毕竟是蒸馏的模型估计还是有点区别，所以体验不太好吧。   
  那除了大语言模型之外还有什么可测的吗？其实就像我开始说的，要说能满足业务，洋垃圾显然是绰绰有余，尤其还是顶尖的洋垃圾，普通的业务甚至都不能让洋垃圾产生瓶颈，新的不就更不可能了😥……   

# 感想
  从上面来看，新服务器真的没什么优势啊，性能提高了一些，但是价格翻几十倍，当然那些洋垃圾当年也是超级贵的东西，只是被淘汰了所以失去了价值……不过说来这个价值也许并不是服务器硬件本身的价值，“服务”也是很值钱的啊，像那个支持服务（比如远程诊断、上门服务，现场响应之类的）就是它贵的原因吧，二手的旧服务器2019年就结束支持了，新的有3年的支持期，能到2027年，不过我感觉在这支持期内恐怕没有能用到的地方啊，服务器还是挺难坏的，它最值钱的地方似乎只能被浪费掉了🥲。所以总的来说只有行业领先的业务，才配得上最新的服务器，小规模的业务还是用二手服务器吧😆。   